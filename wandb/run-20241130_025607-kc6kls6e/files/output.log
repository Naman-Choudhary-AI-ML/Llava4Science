  0%|                                                                                                                               | 0/155 [00:00<?, ?it/s]/home/amulyam/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/amulyam/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/amulyam/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
                                                                                                                                                            
{'loss': 1.3702, 'learning_rate': 4e-05, 'epoch': 0.01}
{'loss': 1.367, 'learning_rate': 8e-05, 'epoch': 0.01}
{'loss': 1.3265, 'learning_rate': 0.00012, 'epoch': 0.02}
{'loss': 1.1907, 'learning_rate': 0.00016, 'epoch': 0.03}
{'loss': 1.3497, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 1.0205, 'learning_rate': 0.00019997806834748456, 'epoch': 0.04}
{'loss': 1.0143, 'learning_rate': 0.00019991228300988585, 'epoch': 0.05}
{'loss': 0.9175, 'learning_rate': 0.00019980267284282717, 'epoch': 0.05}
{'loss': 0.9033, 'learning_rate': 0.00019964928592495045, 'epoch': 0.06}
{'loss': 1.2204, 'learning_rate': 0.00019945218953682734, 'epoch': 0.06}
{'loss': 0.8989, 'learning_rate': 0.0001992114701314478, 'epoch': 0.07}
{'loss': 1.2204, 'learning_rate': 0.00019892723329629887, 'epoch': 0.08}
{'loss': 1.1618, 'learning_rate': 0.0001985996037070505, 'epoch': 0.08}
{'loss': 0.9165, 'learning_rate': 0.0001982287250728689, 'epoch': 0.09}
{'loss': 1.1509, 'learning_rate': 0.00019781476007338058, 'epoch': 0.1}
{'loss': 0.8539, 'learning_rate': 0.00019735789028731604, 'epoch': 0.1}
{'loss': 0.8882, 'learning_rate': 0.0001968583161128631, 'epoch': 0.11}
{'loss': 1.1265, 'learning_rate': 0.00019631625667976583, 'epoch': 0.12}
{'loss': 0.7828, 'learning_rate': 0.00019573194975320673, 'epoch': 0.12}
{'loss': 1.1369, 'learning_rate': 0.00019510565162951537, 'epoch': 0.13}
{'loss': 0.8479, 'learning_rate': 0.00019443763702374812, 'epoch': 0.14}
{'loss': 1.0698, 'learning_rate': 0.00019372819894918915, 'epoch': 0.14}
{'loss': 0.8422, 'learning_rate': 0.00019297764858882514, 'epoch': 0.15}
{'loss': 0.8082, 'learning_rate': 0.00019218631515885006, 'epoch': 0.15}
{'loss': 0.8187, 'learning_rate': 0.0001913545457642601, 'epoch': 0.16}
{'loss': 0.7991, 'learning_rate': 0.00019048270524660196, 'epoch': 0.17}
{'loss': 0.8067, 'learning_rate': 0.0001895711760239413, 'epoch': 0.17}
{'loss': 1.1873, 'learning_rate': 0.00018862035792312147, 'epoch': 0.18}
{'loss': 0.804, 'learning_rate': 0.00018763066800438636, 'epoch': 0.19}
{'loss': 1.0991, 'learning_rate': 0.00018660254037844388, 'epoch': 0.19}
{'loss': 0.803, 'learning_rate': 0.00018553642601605068, 'epoch': 0.2}
{'loss': 0.8119, 'learning_rate': 0.00018443279255020152, 'epoch': 0.21}
{'loss': 0.7868, 'learning_rate': 0.00018329212407100994, 'epoch': 0.21}
{'loss': 1.0481, 'learning_rate': 0.00018211492091337042, 'epoch': 0.22}
{'loss': 0.796, 'learning_rate': 0.00018090169943749476, 'epoch': 0.23}
{'loss': 0.8095, 'learning_rate': 0.00017965299180241963, 'epoch': 0.23}
{'loss': 1.1647, 'learning_rate': 0.000178369345732584, 'epoch': 0.24}
{'loss': 0.7854, 'learning_rate': 0.00017705132427757895, 'epoch': 0.24}
{'loss': 0.7621, 'learning_rate': 0.00017569950556517566, 'epoch': 0.25}
{'loss': 0.8026, 'learning_rate': 0.00017431448254773944, 'epoch': 0.26}
{'loss': 1.1965, 'learning_rate': 0.00017289686274214118, 'epoch': 0.26}
{'loss': 0.745, 'learning_rate': 0.00017144726796328034, 'epoch': 0.27}
{'loss': 0.7909, 'learning_rate': 0.00016996633405133655, 'epoch': 0.28}
{'loss': 0.7451, 'learning_rate': 0.00016845471059286887, 'epoch': 0.28}
{'loss': 1.1937, 'learning_rate': 0.00016691306063588583, 'epoch': 0.29}
{'loss': 0.7601, 'learning_rate': 0.00016534206039901057, 'epoch': 0.3}
{'loss': 0.7952, 'learning_rate': 0.000163742398974869, 'epoch': 0.3}
{'loss': 0.7585, 'learning_rate': 0.00016211477802783103, 'epoch': 0.31}
{'loss': 1.0415, 'learning_rate': 0.0001604599114862375, 'epoch': 0.32}
{'loss': 0.7828, 'learning_rate': 0.00015877852522924732, 'epoch': 0.32}
{'loss': 0.7332, 'learning_rate': 0.0001570713567684432, 'epoch': 0.33}
{'loss': 0.7185, 'learning_rate': 0.00015533915492433443, 'epoch': 0.33}
{'loss': 0.7023, 'learning_rate': 0.00015358267949789966, 'epoch': 0.34}
{'loss': 1.1594, 'learning_rate': 0.00015180270093731303, 'epoch': 0.35}
{'loss': 1.1266, 'learning_rate': 0.00015000000000000001, 'epoch': 0.35}
{'loss': 0.7546, 'learning_rate': 0.00014817536741017152, 'epoch': 0.36}
{'loss': 0.7342, 'learning_rate': 0.00014632960351198618, 'epoch': 0.37}
{'loss': 0.6921, 'learning_rate': 0.00014446351791849276, 'epoch': 0.37}
{'loss': 0.7839, 'learning_rate': 0.00014257792915650728, 'epoch': 0.38}
{'loss': 0.7561, 'learning_rate': 0.00014067366430758004, 'epoch': 0.39}
{'loss': 0.6779, 'learning_rate': 0.0001387515586452103, 'epoch': 0.39}
{'loss': 1.0854, 'learning_rate': 0.00013681245526846783, 'epoch': 0.4}
{'loss': 0.7372, 'learning_rate': 0.00013485720473218154, 'epoch': 0.41}
{'loss': 0.7558, 'learning_rate': 0.00013288666467385833, 'epoch': 0.41}
{'loss': 0.7256, 'learning_rate': 0.00013090169943749476, 'epoch': 0.42}
{'loss': 0.6988, 'learning_rate': 0.00012890317969444716, 'epoch': 0.43}
{'loss': 0.7513, 'learning_rate': 0.00012689198206152657, 'epoch': 0.43}
{'loss': 0.7505, 'learning_rate': 0.0001248689887164855, 'epoch': 0.44}
{'loss': 1.0537, 'learning_rate': 0.00012283508701106557, 'epoch': 0.44}
{'loss': 0.6911, 'learning_rate': 0.00012079116908177593, 'epoch': 0.45}
{'loss': 0.7258, 'learning_rate': 0.00011873813145857249, 'epoch': 0.46}
{'loss': 0.7706, 'learning_rate': 0.00011667687467161024, 'epoch': 0.46}
{'loss': 1.2203, 'learning_rate': 0.00011460830285624118, 'epoch': 0.47}
{'loss': 0.7252, 'learning_rate': 0.00011253332335643043, 'epoch': 0.48}
{'loss': 0.7235, 'learning_rate': 0.00011045284632676536, 'epoch': 0.48}
{'loss': 0.7235, 'learning_rate': 0.00010836778433323158, 'epoch': 0.49}
{'loss': 0.6792, 'learning_rate': 0.00010627905195293135, 'epoch': 0.5}
{'loss': 1.1695, 'learning_rate': 0.00010418756537291996, 'epoch': 0.5}
{'loss': 0.7428, 'learning_rate': 0.0001020942419883357, 'epoch': 0.51}
{'loss': 1.0118, 'learning_rate': 0.0001, 'epoch': 0.52}
{'loss': 0.7408, 'learning_rate': 9.790575801166432e-05, 'epoch': 0.52}
{'loss': 1.0308, 'learning_rate': 9.581243462708006e-05, 'epoch': 0.53}
{'loss': 1.021, 'learning_rate': 9.372094804706867e-05, 'epoch': 0.53}
{'loss': 0.732, 'learning_rate': 9.163221566676847e-05, 'epoch': 0.54}
{'loss': 0.7157, 'learning_rate': 8.954715367323468e-05, 'epoch': 0.55}
{'loss': 0.6691, 'learning_rate': 8.746667664356956e-05, 'epoch': 0.55}
{'loss': 0.694, 'learning_rate': 8.539169714375885e-05, 'epoch': 0.56}
{'loss': 0.6831, 'learning_rate': 8.332312532838978e-05, 'epoch': 0.57}
{'loss': 1.1418, 'learning_rate': 8.126186854142752e-05, 'epoch': 0.57}
{'loss': 0.7186, 'learning_rate': 7.920883091822408e-05, 'epoch': 0.58}
{'loss': 1.133, 'learning_rate': 7.716491298893442e-05, 'epoch': 0.59}
{'loss': 0.7256, 'learning_rate': 7.513101128351454e-05, 'epoch': 0.59}
{'loss': 1.1446, 'learning_rate': 7.310801793847344e-05, 'epoch': 0.6}
{'loss': 0.7323, 'learning_rate': 7.109682030555283e-05, 'epoch': 0.61}
{'loss': 1.0949, 'learning_rate': 6.909830056250527e-05, 'epoch': 0.61}
{'loss': 0.7123, 'learning_rate': 6.711333532614168e-05, 'epoch': 0.62}
{'loss': 0.689, 'learning_rate': 6.51427952678185e-05, 'epoch': 0.62}
{'loss': 0.732, 'learning_rate': 6.318754473153221e-05, 'epoch': 0.63}
{'loss': 1.0345, 'learning_rate': 6.12484413547897e-05, 'epoch': 0.64}
{'loss': 1.0469, 'learning_rate': 5.9326335692419995e-05, 'epoch': 0.64}
{'loss': 0.7139, 'learning_rate': 5.7422070843492734e-05, 'epoch': 0.65}
{'loss': 0.7103, 'learning_rate': 5.553648208150728e-05, 'epoch': 0.66}
{'loss': 0.7218, 'learning_rate': 5.3670396488013854e-05, 'epoch': 0.66}
{'loss': 0.99, 'learning_rate': 5.182463258982846e-05, 'epoch': 0.67}
{'loss': 0.7413, 'learning_rate': 5.000000000000002e-05, 'epoch': 0.68}
{'loss': 0.7086, 'learning_rate': 4.8197299062686995e-05, 'epoch': 0.68}
{'loss': 0.691, 'learning_rate': 4.6417320502100316e-05, 'epoch': 0.69}
{'loss': 1.1121, 'learning_rate': 4.46608450756656e-05, 'epoch': 0.7}
{'loss': 0.7212, 'learning_rate': 4.2928643231556844e-05, 'epoch': 0.7}
{'loss': 0.7288, 'learning_rate': 4.12214747707527e-05, 'epoch': 0.71}
{'loss': 0.7343, 'learning_rate': 3.954008851376252e-05, 'epoch': 0.71}
{'loss': 1.0677, 'learning_rate': 3.788522197216897e-05, 'epoch': 0.72}
{'loss': 0.7342, 'learning_rate': 3.6257601025131026e-05, 'epoch': 0.73}
{'loss': 1.0825, 'learning_rate': 3.465793960098945e-05, 'epoch': 0.73}
{'loss': 0.7359, 'learning_rate': 3.308693936411421e-05, 'epoch': 0.74}
{'loss': 1.0087, 'learning_rate': 3.154528940713113e-05, 'epoch': 0.75}
{'loss': 0.6994, 'learning_rate': 3.0033665948663448e-05, 'epoch': 0.75}
{'loss': 1.0612, 'learning_rate': 2.8552732036719687e-05, 'epoch': 0.76}
{'loss': 0.7385, 'learning_rate': 2.7103137257858868e-05, 'epoch': 0.77}
{'loss': 0.7155, 'learning_rate': 2.5685517452260567e-05, 'epoch': 0.77}
{'loss': 0.7311, 'learning_rate': 2.4300494434824373e-05, 'epoch': 0.78}
{'loss': 0.7324, 'learning_rate': 2.2948675722421086e-05, 'epoch': 0.79}
{'loss': 0.6759, 'learning_rate': 2.163065426741603e-05, 'epoch': 0.79}
{'loss': 0.7413, 'learning_rate': 2.0347008197580374e-05, 'epoch': 0.8}
{'loss': 1.0983, 'learning_rate': 1.9098300562505266e-05, 'epoch': 0.81}
{'loss': 0.7024, 'learning_rate': 1.78850790866296e-05, 'epoch': 0.81}
{'loss': 0.6647, 'learning_rate': 1.6707875928990058e-05, 'epoch': 0.82}
{'loss': 0.711, 'learning_rate': 1.5567207449798515e-05, 'epoch': 0.82}
{'loss': 0.7132, 'learning_rate': 1.4463573983949341e-05, 'epoch': 0.83}
{'loss': 1.1974, 'learning_rate': 1.339745962155613e-05, 'epoch': 0.84}
{'loss': 0.6737, 'learning_rate': 1.2369331995613665e-05, 'epoch': 0.84}
{'loss': 0.702, 'learning_rate': 1.1379642076878527e-05, 'epoch': 0.85}
{'loss': 0.7553, 'learning_rate': 1.042882397605871e-05, 'epoch': 0.86}
{'loss': 0.715, 'learning_rate': 9.517294753398064e-06, 'epoch': 0.86}
{'loss': 0.6731, 'learning_rate': 8.645454235739903e-06, 'epoch': 0.87}
{'loss': 0.7087, 'learning_rate': 7.81368484114996e-06, 'epoch': 0.88}
{'loss': 1.115, 'learning_rate': 7.022351411174866e-06, 'epoch': 0.88}
{'loss': 0.687, 'learning_rate': 6.2718010508108545e-06, 'epoch': 0.89}
{'loss': 0.7142, 'learning_rate': 5.562362976251901e-06, 'epoch': 0.9}
{'loss': 0.6593, 'learning_rate': 4.8943483704846475e-06, 'epoch': 0.9}
{'loss': 0.6943, 'learning_rate': 4.268050246793276e-06, 'epoch': 0.91}
{'loss': 0.6708, 'learning_rate': 3.68374332023419e-06, 'epoch': 0.91}
{'loss': 1.146, 'learning_rate': 3.1416838871368924e-06, 'epoch': 0.92}
{'loss': 1.079, 'learning_rate': 2.6421097126839712e-06, 'epoch': 0.93}
{'loss': 0.6923, 'learning_rate': 2.1852399266194314e-06, 'epoch': 0.93}
{'loss': 0.7053, 'learning_rate': 1.771274927131139e-06, 'epoch': 0.94}
{'loss': 0.6868, 'learning_rate': 1.400396292949513e-06, 'epoch': 0.95}
{'loss': 1.0637, 'learning_rate': 1.0727667037011668e-06, 'epoch': 0.95}
{'loss': 0.7144, 'learning_rate': 7.885298685522235e-07, 'epoch': 0.96}
{'loss': 0.6964, 'learning_rate': 5.478104631726711e-07, 'epoch': 0.97}
{'loss': 1.0603, 'learning_rate': 3.50714075049563e-07, 'epoch': 0.97}
{'loss': 1.1259, 'learning_rate': 1.973271571728441e-07, 'epoch': 0.98}
{'loss': 0.7145, 'learning_rate': 8.771699011416168e-08, 'epoch': 0.99}
{'loss': 1.0222, 'learning_rate': 2.193165251545004e-08, 'epoch': 0.99}
{'loss': 0.9348, 'learning_rate': 0.0, 'epoch': 1.0}
{'train_runtime': 22335.3298, 'train_samples_per_second': 0.445, 'train_steps_per_second': 0.007, 'train_loss': 0.8699749119820134, 'epoch': 1.0}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
/home/amulyam/miniconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
