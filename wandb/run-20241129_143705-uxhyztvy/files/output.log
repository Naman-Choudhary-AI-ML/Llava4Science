  0%|                                                                                                                                | 0/23 [00:00<?, ?it/s]/home/amulyam/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/amulyam/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/amulyam/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [33:24<00:00, 87.15s/it]
{'loss': 1.4736, 'learning_rate': 0.0002, 'epoch': 0.04}
{'loss': 1.4776, 'learning_rate': 0.0001989821441880933, 'epoch': 0.09}
{'loss': 1.1953, 'learning_rate': 0.00019594929736144976, 'epoch': 0.13}
{'loss': 1.0565, 'learning_rate': 0.00019096319953545185, 'epoch': 0.17}
{'loss': 0.9564, 'learning_rate': 0.00018412535328311814, 'epoch': 0.21}
{'loss': 0.9462, 'learning_rate': 0.00017557495743542585, 'epoch': 0.26}
{'loss': 0.9356, 'learning_rate': 0.00016548607339452853, 'epoch': 0.3}
{'loss': 0.912, 'learning_rate': 0.00015406408174555976, 'epoch': 0.34}
{'loss': 0.89, 'learning_rate': 0.00014154150130018866, 'epoch': 0.38}
{'loss': 0.8757, 'learning_rate': 0.00012817325568414297, 'epoch': 0.43}
{'loss': 0.8873, 'learning_rate': 0.00011423148382732853, 'epoch': 0.47}
{'loss': 0.8522, 'learning_rate': 0.0001, 'epoch': 0.51}
{'loss': 0.8455, 'learning_rate': 8.57685161726715e-05, 'epoch': 0.55}
{'loss': 0.8681, 'learning_rate': 7.182674431585704e-05, 'epoch': 0.6}
{'loss': 0.883, 'learning_rate': 5.845849869981137e-05, 'epoch': 0.64}
{'loss': 0.8529, 'learning_rate': 4.593591825444028e-05, 'epoch': 0.68}
{'loss': 0.8666, 'learning_rate': 3.45139266054715e-05, 'epoch': 0.72}
{'loss': 0.8344, 'learning_rate': 2.4425042564574184e-05, 'epoch': 0.77}
{'loss': 0.8258, 'learning_rate': 1.587464671688187e-05, 'epoch': 0.81}
{'loss': 0.8236, 'learning_rate': 9.036800464548157e-06, 'epoch': 0.85}
{'loss': 0.82, 'learning_rate': 4.050702638550275e-06, 'epoch': 0.89}
{'loss': 0.8362, 'learning_rate': 1.0178558119067315e-06, 'epoch': 0.94}
{'loss': 0.8464, 'learning_rate': 0.0, 'epoch': 0.98}
{'train_runtime': 2008.5893, 'train_samples_per_second': 1.494, 'train_steps_per_second': 0.011, 'train_loss': 0.9461374282836914, 'epoch': 0.98}
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
/home/amulyam/miniconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
