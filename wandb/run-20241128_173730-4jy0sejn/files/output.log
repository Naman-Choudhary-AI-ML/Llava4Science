  0%|                                                                                                                               | 0/468 [00:00<?, ?it/s]/home/amulyam/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/amulyam/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/amulyam/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  3%|██▊                                                                                                             | 12/468 [1:04:51<40:39:49, 321.03s/it]
{'loss': 1.3699, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.03}
{'loss': 1.4199, 'learning_rate': 2.6666666666666667e-05, 'epoch': 0.05}
{'loss': 1.3492, 'learning_rate': 4e-05, 'epoch': 0.08}
{'loss': 1.2304, 'learning_rate': 5.333333333333333e-05, 'epoch': 0.1}
{'loss': 1.1698, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.13}
{'loss': 1.0957, 'learning_rate': 8e-05, 'epoch': 0.15}
{'loss': 1.0487, 'learning_rate': 9.333333333333334e-05, 'epoch': 0.18}
{'loss': 0.9446, 'learning_rate': 0.00010666666666666667, 'epoch': 0.21}
{'loss': 0.8468, 'learning_rate': 0.00012, 'epoch': 0.23}
{'loss': 0.7662, 'learning_rate': 0.00013333333333333334, 'epoch': 0.26}
{'loss': 0.709, 'learning_rate': 0.00014666666666666666, 'epoch': 0.28}
{'loss': 0.5607, 'learning_rate': 0.00016, 'epoch': 0.31}
