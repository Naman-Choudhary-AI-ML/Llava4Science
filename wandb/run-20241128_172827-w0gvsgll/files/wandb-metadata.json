{
  "os":  "Linux-3.10.0-957.1.3.el7.x86_64-x86_64-with-glibc2.17",
  "python":  "3.10.15",
  "startedAt":  "2024-11-28T22:28:27.331239Z",
  "args":  [
    "--local_rank=0",
    "--lora_enable",
    "True",
    "--lora_r",
    "128",
    "--lora_alpha",
    "256",
    "--mm_projector_lr",
    "2e-5",
    "--deepspeed",
    "/user_data/amulyam/Projects/LLaVA/FineTune/scripts/zero3_offload.json",
    "--model_name_or_path",
    "liuhaotian/llava-v1.5-7b",
    "--version",
    "v1",
    "--data_path",
    "/user_data/amulyam/Projects/ScienceQA/data/scienceqa/llava_train_QCM-LEA.json_20.json",
    "--image_folder",
    "/user_data/amulyam/Projects/ScienceQA/train",
    "--vision_tower",
    "openai/clip-vit-large-patch14-336",
    "--mm_projector_type",
    "mlp2x_gelu",
    "--mm_vision_select_layer",
    "-2",
    "--mm_use_im_start_end",
    "False",
    "--mm_use_im_patch_token",
    "False",
    "--image_aspect_ratio",
    "pad",
    "--group_by_modality_length",
    "True",
    "--fp16",
    "True",
    "--output_dir",
    "/user_data/amulyam/Projects/LLaVA/scienceQAtune/llava-v1.5-7b",
    "--num_train_epochs",
    "12",
    "--per_device_train_batch_size",
    "4",
    "--per_device_eval_batch_size",
    "4",
    "--gradient_accumulation_steps",
    "4",
    "--evaluation_strategy",
    "no",
    "--save_strategy",
    "steps",
    "--save_steps",
    "50000",
    "--save_total_limit",
    "1",
    "--learning_rate",
    "2e-4",
    "--weight_decay",
    "0.",
    "--warmup_ratio",
    "0.03",
    "--lr_scheduler_type",
    "cosine",
    "--logging_steps",
    "1",
    "--tf32",
    "False",
    "--model_max_length",
    "512",
    "--gradient_checkpointing",
    "True",
    "--dataloader_num_workers",
    "4",
    "--lazy_preprocess",
    "True",
    "--report_to",
    "wandb"
  ],
  "program":  "/user_data/amulyam/Projects/LLaVA/FineTune/train/train.py",
  "codePath":  "train/train.py",
  "git":  {
    "remote":  "https://github.com/Naman-Choudhary-AI-ML/Llava4Science.git",
    "commit":  "900d7b076c44222e53a5e536b7520244c3f762a7"
  },
  "email":  "amulyam@andrew.cmu.edu",
  "root":  "/user_data/amulyam/Projects/LLaVA/FineTune",
  "host":  "mind-0-3.eth",
  "username":  "amulyam",
  "executable":  "/home/amulyam/miniconda3/envs/llava/bin/python",
  "codePathLocal":  "train/train.py",
  "cpu_count":  24,
  "cpu_count_logical":  48,
  "gpu":  "NVIDIA TITAN RTX",
  "gpu_count":  10,
  "disk":  {
    "/":  {
      "total":  "501292785664",
      "used":  "435280949248"
    }
  },
  "memory":  {
    "total":  "270172938240"
  },
  "cpu":  {
    "count":  24,
    "countLogical":  48
  },
  "gpu_nvidia":  [
    {
      "name":  "NVIDIA TITAN RTX",
      "memoryTotal":  "25769803776",
      "cudaCores":  4608,
      "architecture":  "Turing"
    },
    {
      "name":  "NVIDIA TITAN RTX",
      "memoryTotal":  "25769803776",
      "cudaCores":  4608,
      "architecture":  "Turing"
    },
    {
      "name":  "NVIDIA TITAN RTX",
      "memoryTotal":  "25769803776",
      "cudaCores":  4608,
      "architecture":  "Turing"
    },
    {
      "name":  "NVIDIA TITAN RTX",
      "memoryTotal":  "25769803776",
      "cudaCores":  4608,
      "architecture":  "Turing"
    },
    {
      "name":  "NVIDIA TITAN RTX",
      "memoryTotal":  "25769803776",
      "cudaCores":  4608,
      "architecture":  "Turing"
    },
    {
      "name":  "NVIDIA TITAN RTX",
      "memoryTotal":  "25769803776",
      "cudaCores":  4608,
      "architecture":  "Turing"
    },
    {
      "name":  "NVIDIA TITAN RTX",
      "memoryTotal":  "25769803776",
      "cudaCores":  4608,
      "architecture":  "Turing"
    },
    {
      "name":  "NVIDIA TITAN RTX",
      "memoryTotal":  "25769803776",
      "cudaCores":  4608,
      "architecture":  "Turing"
    },
    {
      "name":  "NVIDIA TITAN RTX",
      "memoryTotal":  "25769803776",
      "cudaCores":  4608,
      "architecture":  "Turing"
    },
    {
      "name":  "NVIDIA TITAN RTX",
      "memoryTotal":  "25769803776",
      "cudaCores":  4608,
      "architecture":  "Turing"
    }
  ],
  "cudaVersion":  "12.2"
}