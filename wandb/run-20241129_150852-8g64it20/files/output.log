  0%|                                                                                                                                | 0/51 [00:00<?, ?it/s]/home/amulyam/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/amulyam/miniconda3/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/amulyam/miniconda3/envs/llava/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
                                                                                                                                                            
{'loss': 6.7705, 'learning_rate': 0.0001, 'epoch': 0.02}
{'loss': 6.4031, 'learning_rate': 0.0002, 'epoch': 0.04}
{'loss': 3.7246, 'learning_rate': 0.00019979453927503364, 'epoch': 0.06}
{'loss': 1.6025, 'learning_rate': 0.0001991790013823246, 'epoch': 0.08}
{'loss': 1.2607, 'learning_rate': 0.00019815591569910654, 'epoch': 0.1}
{'loss': 1.5064, 'learning_rate': 0.00019672948630390294, 'epoch': 0.12}
{'loss': 0.6905, 'learning_rate': 0.00019490557470106686, 'epoch': 0.14}
{'loss': 0.8781, 'learning_rate': 0.0001926916757346022, 'epoch': 0.16}
{'loss': 0.7842, 'learning_rate': 0.0001900968867902419, 'epoch': 0.17}
{'loss': 0.7819, 'learning_rate': 0.00018713187041233896, 'epoch': 0.19}
{'loss': 0.7142, 'learning_rate': 0.00018380881048918405, 'epoch': 0.21}
{'loss': 0.5732, 'learning_rate': 0.00018014136218679567, 'epoch': 0.23}
{'loss': 0.6535, 'learning_rate': 0.00017614459583691346, 'epoch': 0.25}
{'loss': 0.7446, 'learning_rate': 0.00017183493500977278, 'epoch': 0.27}
{'loss': 0.7597, 'learning_rate': 0.0001672300890261317, 'epoch': 0.29}
{'loss': 0.5709, 'learning_rate': 0.00016234898018587337, 'epoch': 0.31}
{'loss': 0.5005, 'learning_rate': 0.00015721166601221698, 'epoch': 0.33}
{'loss': 0.6824, 'learning_rate': 0.00015183925683105254, 'epoch': 0.35}
{'loss': 0.6493, 'learning_rate': 0.00014625382902408356, 'epoch': 0.37}
{'loss': 0.6304, 'learning_rate': 0.00014047833431223938, 'epoch': 0.39}
{'loss': 0.5377, 'learning_rate': 0.00013453650544213076, 'epoch': 0.41}
{'loss': 0.4705, 'learning_rate': 0.00012845275866310324, 'epoch': 0.43}
{'loss': 0.6587, 'learning_rate': 0.00012225209339563145, 'epoch': 0.45}
{'loss': 0.6871, 'learning_rate': 0.00011595998950333793, 'epoch': 0.47}
{'loss': 0.5178, 'learning_rate': 0.00010960230259076818, 'epoch': 0.49}
{'loss': 0.6532, 'learning_rate': 0.00010320515775716555, 'epoch': 0.5}
{'loss': 0.4253, 'learning_rate': 9.679484224283449e-05, 'epoch': 0.52}
{'loss': 0.6171, 'learning_rate': 9.039769740923183e-05, 'epoch': 0.54}
{'loss': 0.5332, 'learning_rate': 8.404001049666211e-05, 'epoch': 0.56}
{'loss': 0.537, 'learning_rate': 7.774790660436858e-05, 'epoch': 0.58}
{'loss': 0.603, 'learning_rate': 7.154724133689677e-05, 'epoch': 0.6}
{'loss': 0.5344, 'learning_rate': 6.546349455786926e-05, 'epoch': 0.62}
{'loss': 0.5695, 'learning_rate': 5.952166568776062e-05, 'epoch': 0.64}
{'loss': 0.4229, 'learning_rate': 5.37461709759165e-05, 'epoch': 0.66}
{'loss': 0.4571, 'learning_rate': 4.8160743168947496e-05, 'epoch': 0.68}
{'loss': 0.4223, 'learning_rate': 4.278833398778306e-05, 'epoch': 0.7}
{'loss': 0.4754, 'learning_rate': 3.7651019814126654e-05, 'epoch': 0.72}
{'loss': 0.466, 'learning_rate': 3.276991097386831e-05, 'epoch': 0.74}
{'loss': 0.4191, 'learning_rate': 2.8165064990227252e-05, 'epoch': 0.76}
{'loss': 0.4736, 'learning_rate': 2.3855404163086558e-05, 'epoch': 0.78}
{'loss': 0.43, 'learning_rate': 1.985863781320435e-05, 'epoch': 0.8}
{'loss': 0.4343, 'learning_rate': 1.619118951081594e-05, 'epoch': 0.82}
{'loss': 0.3984, 'learning_rate': 1.286812958766106e-05, 'epoch': 0.83}
{'loss': 0.4361, 'learning_rate': 9.903113209758096e-06, 'epoch': 0.85}
{'loss': 0.3851, 'learning_rate': 7.308324265397836e-06, 'epoch': 0.87}
{'loss': 0.4881, 'learning_rate': 5.094425298933136e-06, 'epoch': 0.89}
{'loss': 0.4471, 'learning_rate': 3.270513696097055e-06, 'epoch': 0.91}
{'loss': 0.5435, 'learning_rate': 1.8440843008934561e-06, 'epoch': 0.93}
{'loss': 0.4552, 'learning_rate': 8.209986176753948e-07, 'epoch': 0.95}
{'loss': 0.4291, 'learning_rate': 2.054607249663665e-07, 'epoch': 0.97}
{'loss': 0.3968, 'learning_rate': 0.0, 'epoch': 0.99}
{'train_runtime': 1736.6529, 'train_samples_per_second': 1.893, 'train_steps_per_second': 0.029, 'train_loss': 0.9059992678025189, 'epoch': 0.99}
Traceback (most recent call last):
  File "/user_data/amulyam/Projects/LLaVA/FineTune/train/train.py", line 991, in <module>
    train()
  File "/user_data/amulyam/Projects/LLaVA/FineTune/train/train.py", line 970, in train
    trainer.save_state()
  File "/home/amulyam/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer_pt_utils.py", line 1043, in save_state
    self.state.save_to_json(path)
  File "/home/amulyam/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer_callback.py", line 114, in save_to_json
    with open(json_path, "w", encoding="utf-8") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/user_data/amulyam/Projects/LLaVA/chesttune/llava-v1.5-7b/trainer_state.json'
Traceback (most recent call last):
  File "/user_data/amulyam/Projects/LLaVA/FineTune/train/train.py", line 991, in <module>
    train()
  File "/user_data/amulyam/Projects/LLaVA/FineTune/train/train.py", line 970, in train
    trainer.save_state()
  File "/home/amulyam/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer_pt_utils.py", line 1043, in save_state
    self.state.save_to_json(path)
  File "/home/amulyam/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/trainer_callback.py", line 114, in save_to_json
    with open(json_path, "w", encoding="utf-8") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/user_data/amulyam/Projects/LLaVA/chesttune/llava-v1.5-7b/trainer_state.json'
